# BERT中英文模型跨领域性能对比分析报告

## 实验概述

本报告分析了基于BERT的中英文模型在不同领域的文本分类性能，包括领域内（In-domain）和跨领域（Out-of-Domain）测试结果。实验涵盖了新闻、网络小说和维基百科等多个领域，通过多个评估指标全面比较了两种语言模型的表现。

## 1. 实验设置

### 1.1 模型配置
- 英文模型：bert-base-uncased
- 训练参数：
  - 批次大小：16
  - 训练轮数：5
  - 学习率：2e-5
  - 权重衰减：0.01
  - 预热比例：0.1
  - 最大序列长度：512

- 中文模型：hfl/rbtl3
- - 训练参数：
  - 批次大小：16
  - 训练轮数：10
  - 学习率：2e-5
  - 权重衰减：0.01
  - 预热比例：0.1
  - 最大序列长度：512

注：zero-shot 方法选取fast-detect-gpt方法，模型分别选择gpt2-xl和Qwen2-0.5B
### 1.2 评估指标
- 准确率（Accuracy）
- 精确率（Precision）
- 召回率（Recall）
- F1分数（F1-score）
- AUROC

## 2. 性能对比分析

### 2.1 监督学习（supervised learning）性能对比

#### 2.1.1 英文领域

源域：essay
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| essay          | 英文 | 0.9824   | 0.9949    | 0.9843 | 0.9896|  0.9950  |
| reuter         | 英文 | 0.9793   | 0.9813    | 0.9948 | 0.9880|  0.9931  |
| wp             | 英文 | 0.7563   | 0.8370    | 0.8854 | 0.8605|  0.3251  |

源域：reuter
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| essay          | 英文 | 0.8491   | 0.8497    | 0.9991 | 0.9184|  0.8235  |
| reuter         | 英文 | 0.8237   | 0.8574    | 0.9527 | 0.9026|  0.6690  |
| wp             | 英文 | 0.8594   | 0.9374    | 0.8941 | 0.9152|  0.8839  |

源域：wp
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| essay          | 英文 | 0.8397   | 0.8993    | 0.9137 | 0.9064|  0.8114  |
| reuter         | 英文 | 0.7771   | 0.9694    | 0.7640 | 0.8546|  0.8762  |
| wp             | 英文 | 0.9699   | 0.9872    | 0.9772 | 0.9822|  0.9881  |

#### 2.1.2 中文领域

源域：news
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| xsum_webnovel  | 中文 | 0.5910   | 0.7220    | 0.2960 | 0.4199|  0.5742  |
| xsum-news      | 中文 | 0.5938   | 0.6094    | 0.5201 | 0.5612|  0.6473  |
| xsum-wiki      | 中文 | 0.6012   | 0.7011    | 0.3527 | 0.4693|  0.7045  |

源域：webnovel
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| xsum_webnovel  | 中文 | 0.8440   | 0.9076    | 0.7660 | 0.8308|  0.9142  |
| xsum-news      | 中文 | 0.5206   | 0.5223    | 0.4699 | 0.4947|  0.5251  |
| xsum-wiki      | 中文 | 0.4719   | 0.4832    | 0.8096 | 0.6052|  0.4163  |

源域：wiki
| 领域           | 语言 | Accuracy | Precision | Recall | F1     |  AUROC  |
|----------------|------|----------|-----------|--------|-------|----------|
| xsum_webnovel  | 中文 | 0.5020   | 0.5018    | 0.5660 | 0.5320|  0.5080  |
| xsum-news      | 中文 | 0.5155   | 0.5136    | 0.5663 | 0.5387|  0.5431  |
| xsum-wiki      | 中文 | 0.6573   | 0.7017    | 0.5471 | 0.6148|  0.7621  |

### 2.2 零样本检测（zero-shot）性能对比
| 领域           | 语言 | 样本数 | Accuracy | Precision | Recall | F1    | Sampling/Scoring Model  |
|----------------|------|--------|----------|-----------|--------|-------|-------------------------|
| essay          | 英文 | 6994   | 0.8450   | 0.9547    | 0.8602 | 0.9050| gpt2-xl/gpt2-xl         |
| reuter         | 英文 | 7000   | 0.7971   | 0.9726    | 0.7855 | 0.8691| gpt2-xl/gpt2-xl         |
| wp             | 英文 | 8000   | 0.8247   | 0.9317    | 0.8630 | 0.8960| gpt2-xl/gpt2-xl         |
| xsum_webnovel  | 中文 | 4997   | 0.8664   | 0.8607    | 0.8743 | 0.8675| Qwen2-0.5B/Qwen2-0.5B   |
| xsum-news      | 中文 | 4530   | 0.8693   | 0.8974    | 0.8340 | 0.8645| Qwen2-0.5B/Qwen2-0.5B   |
| xsum-wiki      | 中文 | 3607   | 0.7040   | 0.9121    | 0.4516 | 0.6041| Qwen2-0.5B/Qwen2-0.5B   |
## 3. 主要发现

### 3.1监督学习模型
#### 3.1.1 领域内性能差异
- 英文模型整体表现显著优于中文模型
- 新闻领域差异最大，英文模型比中文模型高出约38-47个百分点
- 网络小说领域差异相对较小，但英文模型仍然领先7-21个百分点

#### 3.1.2 跨领域泛化能力
- 英文模型的跨领域泛化能力明显优于中文模型
- 英文模型在跨领域时性能下降幅度较小，保持在75-90%之间
- 中文模型跨领域性能显著下降，大多降至50-60%区间

#### 3.1.3 模型稳定性
- 英文模型各项指标表现更加均衡，特别是Precision和Recall的差距较小
- 中文模型各指标波动较大，特别是在跨领域场景下

#### 3.1.4 AUROC表现
- 英文模型的AUROC普遍较高（80-99%），表明分类效果更可靠
- 中文模型AUROC相对较低（52-91%），特别是在跨领域场景

### 3.2零样本检测模型
#### 3.2.1 整体表现对比

- 英文领域(gpt2-xl)：
  - 三个领域指标较为均衡，Accuracy 均在 0.79~0.84，Precision 极高（0.93~0.97），Recall 也较高（0.75~0.84），F1 均高于 0.869。
  - 模型表现稳定，Precision 明显高于 Recall，说明模型对AI文本的识别较为保守（假阳性低，漏检较多），这有可能是因为在英文数据集中ai数据量明显多余人类数据量。

- 中文领域(Qwen2-0.5B)：
  - xsum_webnovel 和 xsum-news 两个领域的各项指标均优于英文，Accuracy 约 0.87，Precision、Recall和F1 很高，说明模型区分能力强。
  - xsum-wiki 域表现下滑明显，Recall 仅 0.45，F1 也只有 0.60，说明 fast-detect-gpt 在此领域区分能力较弱，容易漏检 AI 文本。

#### 3.2.2 模型能力分析

- gpt2-xl（英文）：
  - 在 essay 和 wp 领域表现突出，reuter 略低，整体 Recall 有进一步提升空间。
  - Precision 高于 Recall，说明模型对负例（人类文本）判别更稳健，但对部分 AI 文本判别不出。

- Qwen2-0.5B（中文）：
  - 在 webnovel 和 news 领域表现非常好，Recall 和 Precision 均衡，说明对正负样本区分能力非常强。
  - 在 wiki 域 Recall 明显不足，说明模型对 wiki 域 AI 文本有较高漏检，泛化能力存在不足。

#### 3.2.3 中英文模型对比

- 英文模型的F1分数普遍高于中文模型
- 英文 gpt2-xl 的表现更加稳定，Precision 极高，Recall 稍逊，整体 F1 分数较高，说明其鲁棒性更好。

#### 3.2.4 具体指标对比

- Precision：所有模型都很高，尤其英文领域，意味着模型对AI文本的判别较为严格。
- Recall：中文 wiki 域 Recall 明显低于其它领域，是提升模型泛化能力的重点。
- F1：反映综合能力，英文模型表现普遍高于中文模型。

## 4. 未来的工作

1. 数据质量提升：
   - 增强新闻领域的训练数据质量和数量
   - 确保训练数据的多样性和代表性
   - 优化数据预处理流程

2. 模型优化：
   - 改进跨领域泛化能力的训练策略
   - 优化模型架构以更好处理中文特性
   - 考虑引入领域适应层

## 5. 结论

本研究表明，在监督学习任务中，尽管BERT模型在中英文文本分类任务上都展现出了良好的性能，但在跨领域泛化能力方面仍存在显著差异。英文模型在各个方面都表现出更强的鲁棒性和适应性，而中文模型则在领域迁移时面临更大的挑战；在零样本检测任务中，英文文本分类任务表现性能良好且略强于中文文本，但相比监督学习的结果仍有差距，而中文文本分类任务的性能表现十分优秀，显著强于监督学习的中文文本分类性能，除了wiki数据集，这有可能是wiki数据质量较差的原因。总体而言，零样本检测方法在不同领域的性能上没有非常明显的差距，可以比较好的胜任在不同文本领域的ai文本识别任务。这些发现为未来的模型改进和实际应用提供了重要的参考依据。


